{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8611e79c-df22-496a-9640-0777c9b08c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as skRF\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897eedf2-0fdb-40e4-82e2-3b33afea4999",
   "metadata": {
    "tags": []
   },
   "source": [
    "## THOUGHTS/IDEAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f8a49-d5c1-4635-9cda-5371f65e8c48",
   "metadata": {},
   "source": [
    "8/19\n",
    "\n",
    "- Capatability for multiple different DL models input. Right now the model is VGG16. Can change this in section 3b DL Training\n",
    "- need to have the data put through train_test_split before fitting with the model. Evaluate performance. \n",
    "\n",
    "\n",
    "8/7 \n",
    "\n",
    "- try something like [this article](https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34) that clusters individual images into groups?\n",
    "\n",
    "\n",
    "8/1:  \n",
    "- Based on the 2D plots and general knowledge, do we want any variables that are NOT zero?\n",
    "- There are A LOT of datapoints and its challenging to parse out the differences between each simulation. Try clustering to find more differences between the simulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3dcc58-e187-4542-a3ca-24dcbc24416f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57e1903-ce45-4fc9-af9b-6528a6ce22f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Simulation_Data_Extraction(file_path, DROP_VARS = None): \n",
    "    '''\n",
    "    In:\n",
    "        file_path (str): Path to the simulation data\n",
    "        DROP_VARS (list): List of variables to drop when loading the Xarray simulation data\n",
    "    Out:\n",
    "        ds (Dataset): Xarray dataset\n",
    "    '''\n",
    "    ncfile= netCDF4.Dataset(file_path)\n",
    "    ds = xr.open_dataset(xr.backends.NetCDF4DataStore(ncfile), drop_variables = DROP_VARS)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381b8de-f00c-481f-94c1-3c6b37855850",
   "metadata": {},
   "source": [
    "Uncomment below the different length simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703313e3-30e4-4ca3-b862-a71e2dea306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 36 hour simulation\n",
    "# sim_dir =  \"/discover/nobackup/jli30/scratch/notebooks\"\n",
    "\n",
    "# 156 hour simulation\n",
    "sim_dir = \"/discover/nobackup/alburke2/nu_WRF\"\n",
    "\n",
    "sim_file = \"wrfout_d01_2021-03-13_12:00:00\"\n",
    "sim_44_file = f\"{sim_dir}/lsw44_output/{sim_file}\"\n",
    "sim_55_file = f\"{sim_dir}/lsw55_output/{sim_file}\"\n",
    "sim_77_file = f\"{sim_dir}/lsw77_output/{sim_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d27a45-f0ae-4733-b8fe-5c83f08cf081",
   "metadata": {},
   "source": [
    "Include the desired variables in the \"input_features\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5187d9eb-5fcc-4ed1-8ae9-158cfa0e6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_44_all = Simulation_Data_Extraction(sim_44_file) \n",
    "input_features = ['W', 'RE_HAIL_GSFC'] #, 'ACLHF']\n",
    "data_vars = [i for i in ds_44_all.data_vars] \n",
    "drop_features = np.setdiff1d(data_vars,input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2f979-0968-4689-a523-d8b3a20ef88d",
   "metadata": {},
   "source": [
    "Extract the different simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de03b37-b233-44bc-beaa-1ed7fafa2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_44 = Simulation_Data_Extraction(sim_44_file,drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902cd317-c361-4b39-91c6-3fef18010d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_55 = Simulation_Data_Extraction(sim_55_file,drop_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72341de1-8e47-4c06-a9f2-16130b939d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_77 = Simulation_Data_Extraction(sim_77_file,drop_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86640744-d9c2-4d95-9ecb-04230bdb3a4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plotting 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243b214-67b2-4271-8c03-54b371f834dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_HOUR = 100\n",
    "PLOT_LEVEL = 0\n",
    "VARIABLE = 'RE_HAIL_GSFC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f48d6-f795-4718-84b1-3ee64db51e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18, 8))\n",
    "\n",
    "simulations = [\n",
    "    ds_44[VARIABLE][PLOT_HOUR][PLOT_LEVEL],\n",
    "    ds_55[VARIABLE][PLOT_HOUR][PLOT_LEVEL],\n",
    "    ds_77[VARIABLE][PLOT_HOUR][PLOT_LEVEL],\n",
    "    (ds_44[VARIABLE][PLOT_HOUR][PLOT_LEVEL]-ds_77[VARIABLE][PLOT_HOUR][PLOT_LEVEL]),\n",
    "    (ds_44[VARIABLE][PLOT_HOUR][PLOT_LEVEL]-ds_55[VARIABLE][PLOT_HOUR][PLOT_LEVEL]),\n",
    "    (ds_55[VARIABLE][PLOT_HOUR][PLOT_LEVEL]-ds_77[VARIABLE][PLOT_HOUR][PLOT_LEVEL])\n",
    "]\n",
    "\n",
    "simulation_names = ['44','55','77','44 v. 55', '44 v. 55', '55 v. 77']\n",
    "s = 0\n",
    "for row in np.arange(2):\n",
    "    for col in np.arange(3):\n",
    "        axs[row,col].set_xticks([])\n",
    "        axs[row,col].set_yticks([])\n",
    "        \n",
    "        if row >= 1: plot_map = 'bwr'\n",
    "        else: plot_map = 'viridis' \n",
    "        sim_plot = axs[row,col].imshow(simulations[s], cmap = plot_map)\n",
    "        cbar = fig.colorbar(sim_plot, ax=axs[row,col])\n",
    "        cbar.set_label(ds_44[VARIABLE].units)\n",
    "        axs[row,col].set_title(f'{simulation_names[s]}')\n",
    "        s+=1\n",
    "        \n",
    "plt.suptitle(f'Variable: {VARIABLE}, Hour: {PLOT_HOUR}, Level: {PLOT_LEVEL}',fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30824fd-2965-42f0-93c1-1eee3bdca754",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a9f00-eae8-476e-9c07-4ede90e3da96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2a. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e6c0c-ba8d-4b29-aa88-d96c65a97551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation_Data_Preprocessing(input_data, sim_name, NUM_LEVEL = 40, START_HOUR = 0, END_HOUR = 36, RAND_DATAP = 0.1):\n",
    "    '''\n",
    "    In:\n",
    "        input_data (Dataset): input simulation data\n",
    "        sim_name (str): Name of the simulation. Either '44','55', or '77' for this work\n",
    "        NUM_LEVEL (int): Number of levels desired for a given hour\n",
    "        START_HOUR (int): The first hour input to a list process\n",
    "        END_HOUR (int): The last hour input to a list process\n",
    "        RAND_DATAP (float < 1.0): A float value describing what percentage of the total datapoints is used\n",
    "    Out:\n",
    "        vars_df (DataFrame): 1D chosen datapoints for each variable as a pandas Dataframe\n",
    "        vars_df_col_names (list): List of the different names of each saved variable. May include the specific\n",
    "                                hour or level chosen for a given variable\n",
    "    '''\n",
    "    print(f'Simulation: {sim_name}, Hours: {START_HOUR}-{END_HOUR}')\n",
    "    HOURS=np.arange(START_HOUR,END_HOUR)\n",
    "    \n",
    "    # Create a dictionary with the chosen varaibles as the keys\n",
    "    vars_df_dict = dict.fromkeys(ds_44.data_vars,[])\n",
    "    vars_df_col_names = [] \n",
    "    \n",
    "    for var in ds_44.data_vars:\n",
    "        ################ (SEE CELL BELOW) \n",
    "        # add a step that looks for the mean and std of the chosen hours?\n",
    "        # Only save the datapoints that are outside the 1st sd \n",
    "        # mean, std = Scaling(\n",
    "        #      np.ravel(input_data[var][HOURS]).reshape(-1, 1)\n",
    "        #             )\n",
    "        ################\n",
    "        print(f'\\nVariable: {var}')\n",
    "        all_hour_var = []\n",
    "        for hour in HOURS: \n",
    "            if hour%5 == 0: print(f'Hour: {hour}')\n",
    "            new_var = f'{sim_name}_{var}_h{hour}'\n",
    "            if np.ndim(input_data[var][hour]) > 2:\n",
    "                #data are 3D, stack the different levels into different \"vars\"\n",
    "                level_list = np.arange(len(input_data[var][hour]))\n",
    "                #randomly choose which levels to use\n",
    "                rand_levels = random.choices(level_list,k=NUM_LEVEL)\n",
    "                for level in rand_levels:\n",
    "                    hourly_sim_level = input_data[var][hour][level].values.ravel()\n",
    "                    all_hour_var.append(hourly_sim_level)\n",
    "                    #Save the hourly/level variable data\n",
    "                    hourly_level_name = f'{new_var}_l{level}'\n",
    "                    vars_df_col_names.append(hourly_level_name)\n",
    "            else:\n",
    "                #data are 2D, no extra levels\n",
    "                hourly_sim = input_data[var][hour].values.ravel()\n",
    "                vars_df_col_names.append(new_var)\n",
    "                all_hour_var.append(hourly_sim)\n",
    "        #Save the total hourly/level variable data into one variable        \n",
    "        vars_df_dict[var] = np.ravel(all_hour_var)\n",
    "    #Convert the dictionary to a dataframe\n",
    "    vars_df = pd.DataFrame.from_dict(vars_df_dict)  \n",
    "    \n",
    "    #Remove any hail upflux values == 0\n",
    "    vars_df = vars_df[(vars_df['RE_HAIL_GSFC']!=0)]\n",
    "    \n",
    "    print(f'\\n--------- Processed Simulation {sim_name} Data ---------')\n",
    "    print(vars_df)\n",
    "    \n",
    "    return vars_df, vars_df_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50543d-95e0-49e7-9df7-c8094921f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Scaling(input_data, SIM_NAME = None, SD_FACTOR = 1.0):\n",
    "#     '''\n",
    "#     (Not currently in use but for possible future) \n",
    "#     In:\n",
    "#         input_data (Dataset): input simulation data\n",
    "#         SIM_NAME (str): Name of the simulation. Either '44','55', or '77' for this work\n",
    "#         SD_FACTOR (float): The standard deviation from the mean desired. \n",
    "#             So, 1 sd from the mean, or 2 sd from the mean, etc. \n",
    "#     Out:\n",
    "#         upper_limit (float): Upper limit value greater than the SD_FACTOR from the mean\n",
    "#         lower_limit (float): Lower limit value greater than the SD_FACTOR from the mean\n",
    "\n",
    "#     '''\n",
    "#     start_time = time.time()\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(input_data)\n",
    "#     scaler_mean = scaler.mean_\n",
    "#     scaler_std = np.sqrt(scaler.var_)\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#     print(SIM_NAME,'Mean',scaler_mean,'Sd',scaler_std)\n",
    "#     upper_limit = scaler_mean + (SD_FACTOR*scaler_std)\n",
    "#     lower_limit = scaler_mean - (SD_FACTOR*scaler_std)\n",
    "#     return upper_limit,lower_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f497d-28fb-430f-a2f4-51949aebff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 150\n",
    "processing_params = {\n",
    "    'START_HOUR':start,\n",
    "    'END_HOUR':end, \n",
    "    'NUM_LEVEL':10\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4587d53-e109-4ff9-badd-d5eecafc5db7",
   "metadata": {},
   "source": [
    "Change the starting/ending hours depending on the length of the simulation. \n",
    "Change the number of levels (\"NUM_LEVEL\") randomly chosen per desired hour range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f46d2-0886-4e11-b96d-6d2c6f07d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_44_vars, df_44_image_names = Simulation_Data_Preprocessing(ds_44, '44', **processing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8b2ab-8a79-4759-97a5-0a4d435fd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_55_vars, df_55_image_names = Simulation_Data_Preprocessing(ds_55, '55', **processing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c1369-5ed1-4170-95c2-228777835df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_77_vars, df_77_image_names = Simulation_Data_Preprocessing(ds_77, '77', **processing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6dc7e-a934-452c-b8de-73cf748fa553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1dbb8e-a0b9-401b-9f55-ecfa2339d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 5\n",
    "\n",
    "km_params  = {\n",
    "    'n_clusters':cluster_number, \n",
    "    'n_init':10, \n",
    "    'random_state':42\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999590bc-3867-4694-b841-fcd504d1731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm_pred = GaussianMixture(random_state=42).fit_predict(df_44_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee034d-b3bd-4fb4-b55d-9d2a5f3888f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "km_44_pred = KMeans(**km_params).fit_predict(df_44_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc451d-8040-43aa-8b01-dbbc3b04046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "km_55_pred = KMeans(**km_params).fit_predict(df_55_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73ea2f-2ddd-45fa-8eb4-bf54dc8de72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "km_77_pred = KMeans(**km_params).fit_predict(df_77_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d482c1-a545-41ba-abb7-c03f9ce1154d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bebd90fb-e6e7-45d1-88d3-cbba1fdd6b1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Stratified Cluster Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf938a94-e15f-4cd2-a09a-7955691e18d0",
   "metadata": {},
   "source": [
    "Smallest cluster between 44,55,77\n",
    "\n",
    "Randomly choose the same number of the smallest cluster across the different clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3ef69-ab0f-42fb-8b32-7a59677d3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_Counts(kmeans_output):\n",
    "    '''\n",
    "    In:\n",
    "        kmeans_output (list): Kmeans output of the clusters assigned to each given datapoint\n",
    "    Out:\n",
    "       min_count (int): The minimum number of datapoints across the different clusters\n",
    "    '''\n",
    "    unique, counts = np.unique(kmeans_output, return_counts=True)\n",
    "    min_count = np.nanmin(counts)\n",
    "    print(min_count)\n",
    "    return min_count\n",
    "\n",
    "def Stratified_Cluster_Sampling(min_count,kmeans_output):\n",
    "    '''\n",
    "    In:\n",
    "        min_count (int): The minimum number of datapoints desired\n",
    "        kmeans_output (list): Kmeans output of the clusters assigned to each given datapoint\n",
    "    Out:\n",
    "       stratified_clusters (Array): Clusters with the same chosen min_count of datapoints\n",
    "    '''\n",
    "    stratified_clusters = np.array([])\n",
    "    for cluster in np.arange(np.unique(kmeans_output)):\n",
    "        print(f'Cluster {cluster}')  \n",
    "        cluster_data = np.where(kmeans_output == cluster)[0]\n",
    "        sample_data = np.random.choice(cluster_data,min_count,replace=False)\n",
    "        stratified_clusters = np.append(stratified_clusters, sample_data)\n",
    "    stratified_clusters = stratified_clusters.astype('int')\n",
    "    print(len(stratified_clusters))\n",
    "    return stratified_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891c135-c777-484a-8875-f7808cf4a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_44 = Cluster_Counts(km_44_pred)\n",
    "COUNT_55 = Cluster_Counts(km_55_pred)\n",
    "COUNT_77 = Cluster_Counts(km_77_pred)\n",
    "#Needs the smallest cluster size between the 44, 55, 77 labels \n",
    "SMALLEST_COUNT = np.nanmin([COUNT_44,COUNT_55,COUNT_77])\n",
    "print('Smallest cluster count:', SMALLEST_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fc44b-f56d-4bfb-86df-8f4ab8ab318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_44 = Stratified_Cluster_Sampling(SMALLEST_COUNT,km_44_pred)\n",
    "CLUSTER_55 = Stratified_Cluster_Sampling(SMALLEST_COUNT,km_55_pred)\n",
    "CLUSTER_77 = Stratified_Cluster_Sampling(SMALLEST_COUNT,km_77_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb416b-61e8-466f-a979-3a18ebda50b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759bb6d-9857-42f5-b31f-ac7e931d0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dp = 1000000\n",
    "COLUMNS = 3\n",
    "data_sim_list = [df_44_vars,df_55_vars,df_77_vars] #, df_44_vars])\n",
    "kmean_sim_list = [km_44_pred,km_55_pred,km_77_pred] #, gm_pred])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=COLUMNS, figsize=(20, 5))\n",
    "\n",
    "for col in np.arange(COLUMNS):\n",
    "    sim = data_sim_list[col]\n",
    "    print(sim.shape)\n",
    "\n",
    "    axs[col].scatter(sim.iloc[:num_dp,0],sim.iloc[:num_dp,1],c=kmean_sim_list[col][:num_dp])\n",
    "    axs[col].set_xlabel(ds_44['W'].units)\n",
    "    axs[col].set_ylabel(ds_44['RE_HAIL_GSFC'].units)\n",
    "    \n",
    "plt.suptitle(f'{num_lev} rand lev. Hours: {start} - {end}',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c08a6-eace-4b9d-827b-e1bd847621fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dp = 100000000\n",
    "COLUMNS = 3\n",
    "\n",
    "data_sim_list = [df_44_vars, df_55_vars, df_77_vars]\n",
    "data_cluster_list = [df_44_vars.iloc[CLUSTER_44,:],df_55_vars.iloc[CLUSTER_55,:],df_77_vars.iloc[CLUSTER_77,:]]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=COLUMNS, figsize=(20, 5))\n",
    "\n",
    "for c in np.arange(COLUMNS):\n",
    "    sim = data_sim_list[c]\n",
    "    print(sim.shape)\n",
    "    axs[c].scatter(sim.iloc[:num_dp,0],sim.iloc[:num_dp,1], color='blue',s=1)\n",
    "    \n",
    "    clus = data_cluster_list[c]\n",
    "    print(clus.shape)\n",
    "    axs[c].scatter(clus.iloc[:num_dp,0],clus.iloc[:num_dp,1], color='red',s=1)\n",
    "    \n",
    "    axs[c].set_xlabel(ds_44['W'].units)\n",
    "    axs[c].set_ylabel(ds_44['RE_HAIL_GSFC'].units)\n",
    "    \n",
    "plt.suptitle(f'Cluster (red) v. Random Sample (blue) \\n Hours: {start} - {end}',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fd26d-6175-44f4-aabc-6cff81682720",
   "metadata": {},
   "source": [
    "Frequency histograms for the total and cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535279cb-b99e-4c9d-a235-446fcc4dad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_44_vars.columns\n",
    "print(cols)\n",
    "\n",
    "num_dp = 100000000\n",
    "COLUMNS = 3\n",
    "\n",
    "\n",
    "\n",
    "data_cluster_list = [df_44_vars.iloc[CLUSTER_44,:],\n",
    "                    df_55_vars.iloc[CLUSTER_55,:],\n",
    "                    df_77_vars.iloc[CLUSTER_77,:]]\n",
    "\n",
    "label_list = np.array(['44','55','77'])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "\n",
    "for c in np.arange(4): \n",
    "    if c%2 > 0 : \n",
    "        axs[c].set_xlabel(ds_44['RE_HAIL_GSFC'].units)\n",
    "        if c > 1: \n",
    "            axs[c].set_title('TOTAL RE_HAIL_GSFC')\n",
    "        else: \n",
    "            axs[c].set_title('CLUSTER RE_HAIL_GSFC')\n",
    "    else:\n",
    "        axs[c].set_xlabel(ds_44['W'].units)\n",
    "        if c > 1: \n",
    "            axs[c].set_title('TOTAL W')\n",
    "        else: \n",
    "            axs[c].set_title('CLUSTER W')\n",
    "    \n",
    "    for sim in np.arange(3):\n",
    "        if c < 2: \n",
    "            cluster_var = data_cluster_list[sim].iloc[:,c%2]\n",
    "            print(np.shape(cluster_var))\n",
    "            axs[c].hist(cluster_var, label=label_list[sim], log=True, histtype='step') #,linewidth=2)\n",
    "        else: \n",
    "            sim_var = data_sim_list[sim].iloc[:,c%2]\n",
    "            print(np.shape(sim_var))\n",
    "            axs[c].hist(sim_var, label=label_list[sim], log=True, histtype='step') #,linewidth=2)\n",
    "        \n",
    "    axs[0].set_ylabel('Frequency')\n",
    "\n",
    "    \n",
    "plt.legend()\n",
    "plt.suptitle(f'Hours: {start} - {end}',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c709e06-4fc3-4552-9334-2541bc572459",
   "metadata": {
    "tags": []
   },
   "source": [
    " #### Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5559d-1196-420e-98aa-dc63334ed463",
   "metadata": {},
   "source": [
    "All randomly sampled simulation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87e8e9-232e-4db2-bd32-7b521f593e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_44_55_77 = pd.concat([df_44_vars,df_55_vars,df_77_vars])\n",
    "\n",
    "label_sim = np.hstack((\n",
    "    np.full(len(df_44_vars),0),\n",
    "    np.full(len(df_44_vars),1),\n",
    "    np.full(len(df_44_vars),2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60b264-d736-47dd-9cb5-8b5d539c2292",
   "metadata": {},
   "source": [
    "Even balance cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e4937-9a6e-4f8a-9736-eceae53530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_44_55_77 = pd.concat([df_44_vars.iloc[CLUSTER_44,:], df_55_vars.iloc[CLUSTER_55,:],df_77_vars.iloc[CLUSTER_77,:]])\n",
    "\n",
    "label_clus = np.hstack((\n",
    "    np.full(len(CLUSTER_44),0),\n",
    "    np.full(len(CLUSTER_44),1),\n",
    "    np.full(len(CLUSTER_44),2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24891fb-b7a3-4d26-923c-477279e2f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All sampled pts')\n",
    "print(sim_44_55_77, label_sim)\n",
    "print()\n",
    "print('Clustered pts')\n",
    "print(clus_44_55_77, label_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a9443-a66b-4ceb-8113-edd53fc0d6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c678be1d-d65c-4dd6-8d62-85c4b2a2e861",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2b. Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8727593c-15d3-4ff9-8905-366f7f8f8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation_Data_Preprocessing_2D(input_data, sim_name, input_features): #, NUM_LEVEL = 40, START_HOUR = 0, END_HOUR = 36, RAND_DATAP = 0.1):\n",
    "    '''\n",
    "    In:\n",
    "        input_data (Dataset): input simulation data\n",
    "        sim_name (str): Name of the simulation. Either '44','55', or '77' for this work\n",
    "        input_features (list): The chosen input variables\n",
    "    Out:\n",
    "        vars_2D_list (list): 4D data (variable, examples, x, y) \n",
    "    '''\n",
    "    print(f'Simulation: {sim_name}, Features: {input_features}')\n",
    "    \n",
    "    #W has 60 levels and the hail parameter has 61. To keep consistency, only use 60\n",
    "    desired_levels = 60 \n",
    "    vars_2D_list = []\n",
    "    for var in ds_44.data_vars:\n",
    "        new = input_data[var].values\n",
    "        time,level,x,y = new.shape\n",
    "        if level != desired_levels:\n",
    "            new = new[:,:desired_levels,:,:]\n",
    "        print(var, new.shape)\n",
    "        vars_2D_list.append(new.reshape([time * desired_levels, x, y]))\n",
    "    print(f'Total {np.shape(vars_2D_list)}')\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    \n",
    "    return vars_2D_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e42ecc-2052-4bbf-9f23-4834777dd63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 44, Features: ['W', 'RE_HAIL_GSFC']\n",
      "W (151, 60, 240, 300)\n",
      "RE_HAIL_GSFC (151, 60, 240, 300)\n",
      "Total (2, 9060, 240, 300)\n"
     ]
    }
   ],
   "source": [
    "list2D_44 = Simulation_Data_Preprocessing_2D(ds_44, '44', input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480529c6-870d-4053-b6ee-3b9bfdc49198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 55, Features: ['W', 'RE_HAIL_GSFC']\n",
      "W (151, 60, 240, 300)\n",
      "RE_HAIL_GSFC (151, 60, 240, 300)\n",
      "Total (2, 9060, 240, 300)\n"
     ]
    }
   ],
   "source": [
    "list2D_55 = Simulation_Data_Preprocessing_2D(ds_55, '55', input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cdc700-b169-46ab-b7ab-f0f22419ca6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 77, Features: ['W', 'RE_HAIL_GSFC']\n",
      "W (151, 60, 240, 300)\n",
      "RE_HAIL_GSFC (151, 60, 240, 300)\n",
      "Total (2, 9060, 240, 300)\n"
     ]
    }
   ],
   "source": [
    "list2D_77 = Simulation_Data_Preprocessing_2D(ds_77, '77', input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c206c31-4a38-4f4d-a8c6-d156a307b10c",
   "metadata": {},
   "source": [
    "Combining Data\n",
    "\n",
    "Want: (Num. Vars, All examples, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5358c322-ed92-42d6-9277-be1cbbaebf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 27180, 240, 300)\n",
      "(27180, 240, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "combined2D_predictors = np.hstack([list2D_44,list2D_55,list2D_77])\n",
    "print(np.shape(combined2D_predictors))\n",
    "DL_predictors = np.moveaxis(combined2D_predictors,0,-1)\n",
    "print(np.shape(DL_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49f58852-301d-4548-8351-3fce1bf91a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "print(DL_predictors.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4266d293-0cf7-4641-ac1a-2d7839b7763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27180,)\n"
     ]
    }
   ],
   "source": [
    "#Number of tiles \n",
    "num_examples = np.shape(list2D_77)[1]\n",
    "combined_labels = ['44']*num_examples+['55']*num_examples+['77']*num_examples\n",
    "print(np.shape(combined_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744be99e-35d7-4339-b716-c8af2e9f7fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. AI Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2979c-fcc6-4d93-b145-7f6931e2bfcd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3a. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fe3a6-2522-4843-a0cc-086b67e1c1c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe52f6-c110-4de3-a5cd-28cfd4de51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_search_space={\n",
    "    \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "    \"max_depth\" : [5,10, 30, 50, 80, 90, 100, 110],\n",
    "    \"min_samples_leaf\" : [1, 2, 3, 4, 5],\n",
    "    \"min_samples_split\" : [2, 4, 8, 10],\n",
    "    \"bootstrap\" : [True, False],\n",
    "    \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "\n",
    "list_trees = [75, 100, 125, 150, 175, 200, 250, 300, 400, 500]\n",
    "max_depth = [5, 10, 30, 50, 80, 90, 100, 110]\n",
    "min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "min_samples_split = [2, 4, 8, 10]\n",
    "bootstrap = [True, False]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "TREES_AND_DEPTH_ONLY = False\n",
    "GRID_SEARCH = True\n",
    "\n",
    "def cpu_rf_objective(trial, X, y):\n",
    "    param = {'n_estimators': trial.suggest_categorical('n_estimators', list_trees), \n",
    "           'max_depth':trial.suggest_categorical('max_depth', max_depth), \n",
    "           'min_samples_split':trial.suggest_categorical('min_samples_split', min_samples_split), \n",
    "           'min_samples_leaf':trial.suggest_categorical('min_samples_leaf', min_samples_leaf), \n",
    "           'bootstrap': trial.suggest_categorical('bootstrap', bootstrap),\n",
    "           'criterion':'gini', \n",
    "           #'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-8, 1.0, log=True), \n",
    "           'max_features':trial.suggest_categorical('max_features', max_features), \n",
    "           'max_leaf_nodes':None, \n",
    "           'min_impurity_decrease':0.0, \n",
    "           'oob_score':False, \n",
    "           'n_jobs':-1, \n",
    "           # 'random_state':42, \n",
    "           'verbose':0, \n",
    "           'warm_start':False, \n",
    "           'class_weight':None, \n",
    "           'ccp_alpha':0.0, \n",
    "           'max_samples':None\n",
    "        }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = np.empty(5)\n",
    "    \n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        try:\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        except:\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = skRF(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val, preds, average='weighted') \n",
    "        #using weighted for the multiclass because each class is balanced but shuffled\n",
    "        \n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6b97a-b314-46fe-9507-af69de32e8a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111fc09-eb85-4744-a7d6-8b3399cebcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_search_space={\n",
    "    \"n_estimators\":[50, 75, 100, 150, 175, 200, 250, 300, 500],\n",
    "    \"max_depth\":[30, 35, 40, 45, 50, 75],\n",
    "    \"booster\": [\"gbtree\", \"dart\"]\n",
    "}\n",
    "#     \"n_estimators\": [75, 100, 125, 150, 175, 200, 250, 300, 400, 500],\n",
    "#     \"max_depth\" : [5, 10, 30, 50, 80, 90, 100, 110],\n",
    "#     \"min_samples_leaf\" : [1, 2, 3, 4, 5],\n",
    "#     \"min_samples_split\" : [2, 4, 8, 10],\n",
    "#     \"bootstrap\" : [True, False],\n",
    "#     \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "\n",
    "def cpu_xgb_objective(trial, X, y):\n",
    "    xbg_param = {\n",
    "    \"verbosity\": 0,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [50, 75, 100, 150, 175, 200, 250, 300, 500]),\n",
    "    \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "    \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "    \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0), \n",
    "    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),   \n",
    "    \"max_depth\":  trial.suggest_int(\"max_depth\", 30, 50, step=5),     \n",
    "    # \"min_child_weight\":  trial.suggest_int(\"min_child_weight\", 2, 10), \n",
    "    # \"eta\":  trial.suggest_float(\"eta\", 1e-8, 1.0, log=True), \n",
    "    # \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True), \n",
    "    # \"grow_policy\":  trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = np.empty(5)\n",
    "    \n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        try:\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        except:\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-auc\")\n",
    "        classifier = xgb.XGBClassifier(**xbg_param)\n",
    "        eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "        eval_metric = [\"error\",\"auc\"]\n",
    "        classifier.fit(X_train, \n",
    "                   y_train, \n",
    "                   eval_set=eval_set, \n",
    "                   eval_metric=eval_metric, \n",
    "                   early_stopping_rounds=10, \n",
    "                   callbacks=[pruning_callback],\n",
    "                   verbose=False)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        cv_scores[idx] = f1_score(y_val, preds, average='weighted') \n",
    "        #using weighted for the multiclass because each class is balanced but shuffled\n",
    "        \n",
    "        if cv_scores[idx] == 0.0:\n",
    "            print('Pruning because of 0.0 score.')\n",
    "            return 0.0\n",
    "        print('Fold {}: {}'.format(idx, cv_scores[idx]))\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfcc3e-a0a6-4607-b075-6c4006527bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(obj, X_chosen, y_chosen, search_space, #TEST_TRAIN_RATIO = 0.3, \n",
    "           NN = False, NTRIALS = 2):\n",
    "    \n",
    "    # Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    #     X_chosen, y_chosen, test_size=TEST_TRAIN_RATIO, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "    if GRID_SEARCH:\n",
    "        study = optuna.create_study(study_name='RF Tuning Grid Search', \n",
    "                                    direction='maximize',\n",
    "                                    sampler=optuna.samplers.GridSampler(search_space))\n",
    "    else:\n",
    "        study = optuna.create_study(study_name='RF Tuning',\n",
    "                                    direction='maximize')\n",
    "        \n",
    "    study.optimize(lambda trial: obj(trial, X_chosen, y_chosen), n_trials=NTRIALS, timeout=30*600)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "   \n",
    "     #############################\n",
    "    trials = study.best_trials            \n",
    "    max_trial_score = max([trial.values[0] for trial in trials])\n",
    "    max_trial_params = [trial.params for trial in trials \n",
    "                        if trial.values[0] == max_trial_score][0]\n",
    "    max_trial_params['n_jobs'] = -1\n",
    "    score_print = int(np.round(max_trial_score,4)*1000)\n",
    "    print('\\nMax score:', score_print)\n",
    "    # score_out_file = f'{out_file}_MaxScore{score_print}.pkl'\n",
    "    # print(score_out_file)\n",
    "    \n",
    "    #############################\n",
    "    #Testing\n",
    "    start_time = time.time()\n",
    "    hyperparameters = max_trial_params\n",
    "    hyperparameters['n_jobs'] = -1\n",
    "    print('Using these params:')\n",
    "    print(hyperparameters)\n",
    "    # tuned_classifier = skRF(**hyperparameters)\n",
    "    # tuned_classifier.fit(X_chosen, y_chosen)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155ffd2-be15-4a45-8a1a-2e2d263448fe",
   "metadata": {},
   "source": [
    "#### Tuning/Training\n",
    "\n",
    "Which dataset to use first?\n",
    "\n",
    "large sample: sim_44_55_77, label_sim\n",
    "clustered sample: clus_44_55_77, label_clus\n",
    "\n",
    "\n",
    "def tuning(partial_obj, X_chosen, y_chosen, TEST_TRAIN_RATIO = 0.3, NN = False, NTRIALS = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb887d69-487e-4301-a197-49389aa4f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning(cpu_xgb_objective,clus_44_55_77,label_clus, search_space = xgb_search_space, NTRIALS = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036e8d-30f8-4ec9-b2c4-e34703ef31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning(cpu_rf_objective,clus_44_55_77,label_clus, rf_search_space, NTRIALS = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5f7db-f09f-43fc-ab97-d660a24a57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning(cpu_xgb_objective,clus_44_55_77,label_clus, NTRIALS = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ed332-16d8-4978-b290-516bffa19d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2dba1f-2c6e-479c-ad0a-f8b42698e8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3b. DL Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76424393-f533-4e1f-820e-524a503f653e",
   "metadata": {},
   "source": [
    "- predictors: (examples, x, y, variables)\n",
    "- labels: (examples, 3) multiclass classification so 001/010/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb22a4f2-d093-4d87-8013-35a4d9988596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27180, 3)\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "multiclass_labels = lb.fit_transform(combined_labels)\n",
    "print(multiclass_labels.shape)\n",
    "print(multiclass_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db839f80-be2e-4bd0-9941-91a02e765aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 240, 300, 2)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 240, 300, 64)      1216      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 240, 300, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 120, 150, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 120, 150, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 120, 150, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 60, 75, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 60, 75, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 60, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 60, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 30, 37, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 30, 37, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 30, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 30, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 18, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 9, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32256)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              132124672 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 3)                 12291     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163,632,387\n",
      "Trainable params: 163,632,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the base model, no pre-trained weights\n",
    "dl_model = VGG16(weights=None, include_top=True, classes=3, input_shape = DL_predictors.shape[1:])\n",
    "dl_model.compile(optimizer='Adam', loss='categorical_crossentropy')\n",
    "print(dl_model.summary())\n",
    "dl_model.fit(\n",
    "    DL_predictors,\n",
    "    multiclass_labels,\n",
    "    batch_size=128,\n",
    "    epochs=3,\n",
    "    validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22adc0f-0342-452d-bb4b-2c6cf5fdf444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce642e-0c91-4b60-aa09-c5c494372854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b430a33c-ec5a-4cb1-8de2-3f93c5b0c71b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Previous Empirical Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a54302-209b-4aa5-af33-502770c102bb",
   "metadata": {},
   "source": [
    "choosing the 3D levels by the frequency of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc16f3-1b3e-4cbf-a9c9-a5221ac600a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculating_frequency(input_data,round_by=3):\n",
    "#     '''\n",
    "#     input_data (array) : input simulation data, assumed a single variable\n",
    "#     round_by (int): how many decimals are wanted when calculating the frequency of given floats\n",
    "#     '''\n",
    "    \n",
    "#     round_input_1D = np.round(np.ravel(input_data),round_by)\n",
    "#     unique, counts = np.unique(round_input_1D, return_counts=True)\n",
    "#     unique_counts_2D = np.asarray((unique, counts)).T\n",
    "    \n",
    "#     max_freq_ind = np.argmax(unique_counts_2D[:,1])\n",
    "#     max_value_freq = unique_counts_2D[max_freq_ind][1]\n",
    "    \n",
    "#     min_freq_ind = np.argmin(unique_counts_2D[:,1])\n",
    "#     min_freq_value = unique_counts_2D[min_freq_ind][0]\n",
    "   \n",
    "#     return min_freq_value, max_value_freq\n",
    "\n",
    "\n",
    "\n",
    "# def preprocessing(input_sim, sim_name, START_HOUR = 0, END_HOUR = 36):\n",
    "#     print(f'Simulation: {sim_name}, Hours: {START_HOUR}-{END_HOUR}')\n",
    "#     HOURS=np.arange(START_HOUR,END_HOUR)\n",
    "#     vars_df_dict = dict.fromkeys(ds_44.data_vars,[])\n",
    "#     vars_df_col_names = [] \n",
    "#     for hour in HOURS: \n",
    "#         print(f'Hour: {hour}')\n",
    "#         for var in ds_44.data_vars:\n",
    "#             new_var = f'{sim_name}_{var}_h{hour}'\n",
    "#             if np.ndim(input_sim[var][hour]) > 2:\n",
    "#                 #data are 3D, stack the different levels into different \"vars\"\n",
    "#                 # Found that the 2 images that show the most change are the level\n",
    "#                 # with the most frequency of the max value and\n",
    "#                 # level with lowset value of the minimum frequency\n",
    "#                 ######################\n",
    "#                 #minfv aka minimum frequency value \n",
    "#                 minimum_freq_value = 100.0\n",
    "#                 minfv_level = None\n",
    "#                 minfv_var_name = None\n",
    "#                 ######################\n",
    "#                 #maxvf aka maximum value frequency\n",
    "#                 maximum_value_freq = -100.0\n",
    "#                 maxvf_level = None\n",
    "#                 maxvf_var_name = None\n",
    "#                 ######################\n",
    "#                 num_levels = np.shape(input_sim[var][hour])[0]\n",
    "#                 for level in np.arange(num_levels):\n",
    "#                     hourly_sim_level = input_sim[var][hour][level].values\n",
    "#                     hourly_sim_shape = hourly_sim_level.shape[0]*hourly_sim_level.shape[1]\n",
    "#                     hourly_lev_minfv, hourly_lev_maxvf = freq_stuff(hourly_sim_level,sim_name)\n",
    "#                     if (hourly_lev_minfv < minimum_freq_value) & (hourly_lev_minfv != 0): \n",
    "#                         #Dont want the minimum value to be 0 when the values may only start at 0\n",
    "#                         minimum_freq_value = hourly_lev_minfv\n",
    "#                         minfv_level = level\n",
    "#                         minfv_var_name = f'{new_var}_l{level}_minfv'\n",
    "                    \n",
    "#                     if (hourly_lev_maxvf > maximum_value_freq) & (hourly_lev_maxvf != hourly_sim_shape): \n",
    "#                         # Dont want the maximum frequency to just pick the image with all the same value aka blank image\n",
    "#                         maximum_value_freq = hourly_lev_maxvf\n",
    "#                         maxvf_level = level \n",
    "#                         maxvf_var_name = f'{new_var}_l{level}_maxvf' \n",
    "#                 ######################\n",
    "#                 hourly_minfv = input_sim[var][hour][minfv_level].values.ravel()\n",
    "#                 vars_df_col_names.append(minfv_var_name)   \n",
    "#                 vars_df_dict[var].extend(hourly_minfv)\n",
    "#                 ######################\n",
    "#                 hourly_maxvf = input_sim[var][hour][maxvf_level].values.ravel()\n",
    "#                 vars_df_col_names.append(maxvf_var_name)\n",
    "#                 vars_df_dict[var].extend(hourly_maxvf)\n",
    "#                 ######################\n",
    "#             else:\n",
    "#                 #data are 2D, no extra levels\n",
    "#                 hourly_sim = input_sim[var][hour].values.ravel()\n",
    "#                 # vars_df_list_data.append(hourly_sim)\n",
    "#                 vars_df_col_names.append(new_var)\n",
    "#                 vars_df_dict[var].extend(hourly_sim)\n",
    "                \n",
    "#     vars_df = pd.DataFrame.from_dict(vars_df_dict)\n",
    "#     vars_df['label'] = np.full(len(vars_df.iloc[:,0]),sim_name)     \n",
    "#     print(vars_df)\n",
    "    \n",
    "#     return vars_df, vars_df_col_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab]",
   "language": "python",
   "name": "conda-env-ilab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
